{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to append data to csv\n",
    "def save_entry(filename,extra_df):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "    except:\n",
    "        df = pd.DataFrame()\n",
    "    df = pd.concat([df,extra_df])\n",
    "\n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model to use, only one is used at a time, others must be commented.\n",
    "# filename defines where output is saved.\n",
    "\n",
    "\n",
    "# simple CNN classifier\n",
    "\"\"\"def MNIST_classifier(ishape = (28,28,1),k = 10, lr = 1e-4):\n",
    "    model_input = tf.keras.layers.Input(shape = ishape)\n",
    "    l1 = tf.keras.layers.Conv2D(14,3,padding=\"same\",activation=\"relu\")(model_input)\n",
    "    l2 = tf.keras.layers.MaxPool2D(2)(l1)\n",
    "    l3 = tf.keras.layers.Conv2D(28,3,padding=\"same\",activation=\"relu\")(l2)\n",
    "    flatten = tf.keras.layers.Flatten()(l3)\n",
    "    out = tf.keras.layers.Dense(k,activation = 'softmax')(flatten)\n",
    "    model = tf.keras.models.Model(inputs = model_input, outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    return model \n",
    "filename=\"simple_cnn_results.csv\"    \n",
    "\"\"\"\n",
    "\n",
    "# more complex CNN classifier\n",
    "def MNIST_classifier(ishape = (28,28,1),k = 10, lr = 1e-4):\n",
    "    model_input = tf.keras.layers.Input(shape = ishape)\n",
    "    l1 = tf.keras.layers.Conv2D(48,3,padding=\"same\",activation=\"relu\")(model_input)\n",
    "    l2 = tf.keras.layers.Conv2D(48,3,padding=\"same\",activation=\"relu\")(l1)\n",
    "    l2_drop = tf.keras.layers.Dropout(0.25)(l2)\n",
    "    l3 = tf.keras.layers.MaxPool2D(2)(l2_drop)\n",
    "    l4 = tf.keras.layers.Conv2D(96,3,padding=\"same\",activation=\"relu\")(l3)\n",
    "    l5 = tf.keras.layers.Conv2D(96,3,padding=\"same\",activation=\"relu\")(l4)\n",
    "    l5_drop = tf.keras.layers.Dropout(0.25)(l5)\n",
    "    flatten = tf.keras.layers.Flatten()(l5_drop)\n",
    "    out = tf.keras.layers.Dense(k,activation = 'softmax')(flatten)\n",
    "    model = tf.keras.models.Model(inputs = model_input, outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    return model \n",
    "filename = \"cnn_results.csv\"\n",
    "\n",
    "\n",
    "\n",
    "#simple dense layer model\n",
    "\"\"\"def MNIST_classifier(ishape = (28,28,1),k = 10, lr = 1e-4):\n",
    "    model_input = tf.keras.layers.Input(shape = ishape)\n",
    "    flatten = tf.keras.layers.Flatten()(model_input)\n",
    "    out = tf.keras.layers.Dense(k,activation = 'softmax')(flatten)\n",
    "    model = tf.keras.models.Model(inputs = model_input, outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    return model \n",
    "filename = \"dense_results.csv\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m = MNIST_classifier()\n",
    "weights = m.get_weights().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mnist_classifier\"\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
    "\n",
    "monitor = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss',\\\n",
    "                                             verbose=0,save_best_only=True,\\\n",
    "                                             save_weights_only=True,\\\n",
    "                                             mode='min')\n",
    "# Learning rate schedule\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch%10 == 0:\n",
    "        lr = lr/2\n",
    "    return lr\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "750\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3105 - accuracy: 0.9171\n",
      "--------------- 1000 --------------------\n",
      "Categorical cross-entropy: 0.3105418086051941\n",
      "Accuracy: 0.9171000123023987\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "1500\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2214 - accuracy: 0.9412\n",
      "--------------- 2000 --------------------\n",
      "Categorical cross-entropy: 0.2213897407054901\n",
      "Accuracy: 0.9412000179290771\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "2250\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.1524 - accuracy: 0.9571\n",
      "--------------- 3000 --------------------\n",
      "Categorical cross-entropy: 0.15244020521640778\n",
      "Accuracy: 0.957099974155426\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "3000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1265 - accuracy: 0.9626\n",
      "--------------- 4000 --------------------\n",
      "Categorical cross-entropy: 0.12645775079727173\n",
      "Accuracy: 0.9625999927520752\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "3750\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1048 - accuracy: 0.9676\n",
      "--------------- 5000 --------------------\n",
      "Categorical cross-entropy: 0.10477849096059799\n",
      "Accuracy: 0.9675999879837036\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "4500\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0958 - accuracy: 0.9739\n",
      "--------------- 6000 --------------------\n",
      "Categorical cross-entropy: 0.09580940008163452\n",
      "Accuracy: 0.9739000201225281\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "5250\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0895 - accuracy: 0.9737\n",
      "--------------- 7000 --------------------\n",
      "Categorical cross-entropy: 0.08951970189809799\n",
      "Accuracy: 0.9736999869346619\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "6000\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0849 - accuracy: 0.9759\n",
      "--------------- 8000 --------------------\n",
      "Categorical cross-entropy: 0.08488830178976059\n",
      "Accuracy: 0.9758999943733215\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "6750\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0768 - accuracy: 0.9772\n",
      "--------------- 9000 --------------------\n",
      "Categorical cross-entropy: 0.07679092139005661\n",
      "Accuracy: 0.9771999716758728\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "7500\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0744 - accuracy: 0.9793\n",
      "--------------- 10000 --------------------\n",
      "Categorical cross-entropy: 0.07442411035299301\n",
      "Accuracy: 0.9793000221252441\n"
     ]
    }
   ],
   "source": [
    "# list to fill output\n",
    "out_sizes=[]\n",
    "out_accuracies=[]\n",
    "\n",
    "#Experiment parameters:\n",
    "sample_counts = [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "size_increase = 0\n",
    "epoch_count = 30\n",
    "####-------------------\n",
    "\n",
    "\n",
    "# iterates through sample sizes and trains classifier for each of them, \n",
    "# resetting weights\n",
    "for size in sample_counts:\n",
    "    #load data\n",
    "    X_dev = np.load(\"data/base_data/MNIST_X_dev_{}_data.npy\".format(int(size)))\n",
    "    y_dev = np.load(\"data/base_data/MNIST_y_dev_{}_data.npy\".format(int(size)))\n",
    "\n",
    "    #load generator\n",
    "    gen_model = tf.keras.models.load_model(f\"gan_models/trained_gen_{size}.h5\")\n",
    "\n",
    "    #create noise and labels\n",
    "    noise = tf.random.normal(shape=(int(size*size_increase/100), 128))\n",
    "\n",
    "    noise_pre_labels = np.random.choice(y_dev,size=noise.shape[0])\n",
    "    noise_labels = tf.keras.utils.to_categorical(noise_pre_labels,10)\n",
    "\n",
    "    # exception for no increase\n",
    "    if size_increase>0:\n",
    "      fake_images = gen_model.predict([noise,noise_labels])\n",
    "    else:\n",
    "      fake_images=np.array([])\n",
    "\n",
    "\n",
    "    # reshape fake images and increase dataset\n",
    "    fake_images = tf.reshape(fake_images,(-1,28,28))\n",
    "\n",
    "    new_X_dev = np.array(tf.concat([X_dev,fake_images],axis=0))\n",
    "    new_y_dev = np.array(tf.concat([y_dev,noise_pre_labels],axis=0))\n",
    "\n",
    "    #shuffle new dataset\n",
    "    new_indexes = np.arange(new_X_dev.shape[0],dtype=int)\n",
    "    np.random.shuffle(new_indexes)\n",
    "    new_X_dev = new_X_dev[new_indexes]\n",
    "    new_y_dev = new_y_dev[new_indexes]\n",
    "\n",
    "    X_test = np.load(\"data/MNIST_X_test.npy\")\n",
    "    y_test = np.load(\"data/MNIST_y_test.npy\")\n",
    "\n",
    "\n",
    "    #divide train/split\n",
    "    n_split = int(0.75*new_X_dev.shape[0])\n",
    "    print(n_split)\n",
    "    X_train = new_X_dev[:n_split]\n",
    "    X_val = new_X_dev[n_split:]\n",
    "    y_train = new_y_dev[:n_split]\n",
    "    y_val = new_y_dev[n_split:]\n",
    "\n",
    "    #one hot encoding\n",
    "    y_train_oh = tf.keras.utils.to_categorical(y_train)\n",
    "    y_val_oh = tf.keras.utils.to_categorical(y_val)\n",
    "    y_test_oh = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    " \n",
    "    #define model and reset weights\n",
    "    m = MNIST_classifier()\n",
    "    m.set_weights(weights)\n",
    "\n",
    "    # train\n",
    "    m.fit(X_train,y_train_oh,batch_size = 32, epochs = 50, \\\n",
    "          verbose = 0, callbacks= [early_stop, monitor, lr_schedule],validation_data=(X_val,y_val_oh))\n",
    "\n",
    "    # evaluate and append results\n",
    "    metrics = m.evaluate(X_test,y_test_oh)\n",
    "    print(f\"--------------- {size} --------------------\")\n",
    "    print(\"Categorical cross-entropy:\", metrics[0])\n",
    "    print(\"Accuracy:\", metrics[1])\n",
    "\n",
    "    out_sizes.append(size)\n",
    "    out_accuracies.append(metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df to add to file\n",
    "extra_df = pd.DataFrame({\n",
    "        \"original_size\":out_sizes,\n",
    "        \"new_size\":np.array(out_sizes)*(1+size_increase/100),\n",
    "        \"size_increase\":size_increase/100,\n",
    "        \"accuracy\":out_accuracies,\n",
    "        \"training_epochs\":epoch_count\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to file\n",
    "save_entry(filename,extra_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f1fa92065e83bf8b3f9b5096216ce92755ea229f7fd6780b8b3e0f199343520"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
