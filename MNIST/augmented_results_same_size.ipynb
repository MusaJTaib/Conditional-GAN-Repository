{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to append data to csv\n",
    "def save_entry(filename,extra_df):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "    except:\n",
    "        df = pd.DataFrame()\n",
    "    df = pd.concat([df,extra_df])\n",
    "\n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model to use, only one is used at a time, others must be commented.\n",
    "# classifier name defines a column in the output.\n",
    "\n",
    "\n",
    "# simple CNN classifier\n",
    "\"\"\"def MNIST_classifier(ishape = (28,28,1),k = 10, lr = 1e-4):\n",
    "    model_input = tf.keras.layers.Input(shape = ishape)\n",
    "    l1 = tf.keras.layers.Conv2D(14,3,padding=\"same\",activation=\"relu\")(model_input)\n",
    "    l2 = tf.keras.layers.MaxPool2D(2)(l1)\n",
    "    l3 = tf.keras.layers.Conv2D(28,3,padding=\"same\",activation=\"relu\")(l2)\n",
    "    flatten = tf.keras.layers.Flatten()(l3)\n",
    "    out = tf.keras.layers.Dense(k,activation = 'softmax')(flatten)\n",
    "    model = tf.keras.models.Model(inputs = model_input, outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    return model \n",
    "classifier_name=\"simple_cnn\"    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# more complex CNN classifier\n",
    "\"\"\"def MNIST_classifier(ishape = (28,28,1),k = 10, lr = 1e-4):\n",
    "    model_input = tf.keras.layers.Input(shape = ishape)\n",
    "    l1 = tf.keras.layers.Conv2D(48,3,padding=\"same\",activation=\"relu\")(model_input)\n",
    "    l2 = tf.keras.layers.Conv2D(48,3,padding=\"same\",activation=\"relu\")(l1)\n",
    "    l2_drop = tf.keras.layers.Dropout(0.25)(l2)\n",
    "    l3 = tf.keras.layers.MaxPool2D(2)(l2_drop)\n",
    "    l4 = tf.keras.layers.Conv2D(96,3,padding=\"same\",activation=\"relu\")(l3)\n",
    "    l5 = tf.keras.layers.Conv2D(96,3,padding=\"same\",activation=\"relu\")(l4)\n",
    "    l5_drop = tf.keras.layers.Dropout(0.25)(l5)\n",
    "    flatten = tf.keras.layers.Flatten()(l5_drop)\n",
    "    out = tf.keras.layers.Dense(k,activation = 'softmax')(flatten)\n",
    "    model = tf.keras.models.Model(inputs = model_input, outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    return model \n",
    "classifier_name = \"cnn\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#simple dense layer model\n",
    "def MNIST_classifier(ishape = (28,28,1),k = 10, lr = 1e-4):\n",
    "    model_input = tf.keras.layers.Input(shape = ishape)\n",
    "    flatten = tf.keras.layers.Flatten()(model_input)\n",
    "    out = tf.keras.layers.Dense(k,activation = 'softmax')(flatten)\n",
    "    model = tf.keras.models.Model(inputs = model_input, outputs = out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    return model \n",
    "classifier_name=\"dense\"    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m = MNIST_classifier()\n",
    "print(m.summary())\n",
    "weights = m.get_weights().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mnist_classifier\"\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
    "\n",
    "monitor = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss',\\\n",
    "                                             verbose=0,save_best_only=True,\\\n",
    "                                             save_weights_only=True,\\\n",
    "                                             mode='min')\n",
    "# Learning rate schedule\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch%10 == 0:\n",
    "        lr = lr/2\n",
    "    return lr\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(9000, 128)\n",
      "(9000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 988us/step - loss: 16.6319 - accuracy: 0.7389\n",
      "--------------- 1000 --------------------\n",
      "Categorical cross-entropy: 16.631853103637695\n",
      "Accuracy: 0.7389000058174133\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(8000, 128)\n",
      "(8000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 910us/step - loss: 12.9059 - accuracy: 0.7772\n",
      "--------------- 2000 --------------------\n",
      "Categorical cross-entropy: 12.90591812133789\n",
      "Accuracy: 0.7771999835968018\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(7000, 128)\n",
      "(7000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 941us/step - loss: 11.3544 - accuracy: 0.7935\n",
      "--------------- 3000 --------------------\n",
      "Categorical cross-entropy: 11.354387283325195\n",
      "Accuracy: 0.7935000061988831\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(6000, 128)\n",
      "(6000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 898us/step - loss: 10.3282 - accuracy: 0.8073\n",
      "--------------- 4000 --------------------\n",
      "Categorical cross-entropy: 10.328168869018555\n",
      "Accuracy: 0.8072999715805054\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(5000, 128)\n",
      "(5000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 962us/step - loss: 9.6277 - accuracy: 0.8122\n",
      "--------------- 5000 --------------------\n",
      "Categorical cross-entropy: 9.627664566040039\n",
      "Accuracy: 0.8122000098228455\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(4000, 128)\n",
      "(4000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 920us/step - loss: 9.0652 - accuracy: 0.8180\n",
      "--------------- 6000 --------------------\n",
      "Categorical cross-entropy: 9.065192222595215\n",
      "Accuracy: 0.8180000185966492\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(3000, 128)\n",
      "(3000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 922us/step - loss: 8.6288 - accuracy: 0.8252\n",
      "--------------- 7000 --------------------\n",
      "Categorical cross-entropy: 8.62879467010498\n",
      "Accuracy: 0.8252000212669373\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(2000, 128)\n",
      "(2000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 923us/step - loss: 8.4762 - accuracy: 0.8285\n",
      "--------------- 8000 --------------------\n",
      "Categorical cross-entropy: 8.476210594177246\n",
      "Accuracy: 0.828499972820282\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(1000, 128)\n",
      "(1000, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 900us/step - loss: 8.4778 - accuracy: 0.8286\n",
      "--------------- 9000 --------------------\n",
      "Categorical cross-entropy: 8.477838516235352\n",
      "Accuracy: 0.8285999894142151\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "(0, 128)\n",
      "(0, 10)\n",
      "7500\n",
      "313/313 [==============================] - 0s 914us/step - loss: 8.4783 - accuracy: 0.8284\n",
      "--------------- 10000 --------------------\n",
      "Categorical cross-entropy: 8.47825813293457\n",
      "Accuracy: 0.8284000158309937\n"
     ]
    }
   ],
   "source": [
    "# list to fill output\n",
    "out_sizes=[]\n",
    "out_accuracies=[]\n",
    "out_size_increases=[]\n",
    "\n",
    "#Experiment parameters:\n",
    "target_size = 10000\n",
    "sample_counts = [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "\n",
    "epoch_count = 30\n",
    "####-------------------\n",
    "\n",
    "# iterates through sample sizes and trains classifier for each of them, \n",
    "# increasing the dataset size until the target size and\n",
    "# resetting weights\n",
    "for size in sample_counts:\n",
    "\n",
    "    size_increase = target_size-size\n",
    "\n",
    "\n",
    "    #load data\n",
    "    X_dev = np.load(\"data/base_data/MNIST_X_dev_{}_data.npy\".format(int(size)))\n",
    "    y_dev = np.load(\"data/base_data/MNIST_y_dev_{}_data.npy\".format(int(size)))\n",
    "\n",
    "    #load generator\n",
    "    gen_model = tf.keras.models.load_model(f\"gan_models/trained_gen_{size}.h5\")\n",
    "\n",
    "\n",
    "    #create noise and labels\n",
    "    noise = tf.random.normal(shape=(int(size_increase), 128))\n",
    "\n",
    "    noise_pre_labels = np.random.choice(y_dev,size=noise.shape[0])\n",
    "    noise_labels = tf.keras.utils.to_categorical(noise_pre_labels,10)\n",
    "\n",
    "    # exception for no increase\n",
    "    if size_increase>0:\n",
    "      fake_images = gen_model.predict([noise,noise_labels])\n",
    "\n",
    "    else:\n",
    "      fake_images=np.array([])\n",
    "\n",
    "    # reshape fake images and increase dataset\n",
    "    fake_images = tf.reshape(fake_images,(-1,28,28))\n",
    "\n",
    "    new_X_dev = np.array(tf.concat([X_dev,fake_images],axis=0))\n",
    "    new_y_dev = np.array(tf.concat([y_dev,noise_pre_labels],axis=0))\n",
    "\n",
    "\n",
    "    #shuffle new dataset\n",
    "    new_indexes = np.arange(new_X_dev.shape[0],dtype=int)\n",
    "    np.random.shuffle(new_indexes)\n",
    "    new_X_dev = new_X_dev[new_indexes]\n",
    "    new_y_dev = new_y_dev[new_indexes]\n",
    "\n",
    "    X_test = np.load(\"data/MNIST_X_test.npy\")\n",
    "    y_test = np.load(\"data/MNIST_y_test.npy\")\n",
    "\n",
    "    #divide train/split\n",
    "    n_split = int(0.75*new_X_dev.shape[0])\n",
    "    print(n_split)\n",
    "    X_train = new_X_dev[:n_split]\n",
    "    X_val = new_X_dev[n_split:]\n",
    "    y_train = new_y_dev[:n_split]\n",
    "    y_val = new_y_dev[n_split:]\n",
    "\n",
    "    #one hot encoding\n",
    "    y_train_oh = tf.keras.utils.to_categorical(y_train)\n",
    "    y_val_oh = tf.keras.utils.to_categorical(y_val)\n",
    "    y_test_oh = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    " \n",
    "    #define model and reset weights\n",
    "    m = MNIST_classifier()\n",
    "    m.set_weights(weights)\n",
    "\n",
    "    # train\n",
    "    m.fit(X_train,y_train_oh,batch_size = 32, epochs = 50, \\\n",
    "          verbose = 0, callbacks= [early_stop, monitor, lr_schedule],validation_data=(X_val,y_val_oh))\n",
    "\n",
    "    # evaluate and append results\n",
    "    metrics = m.evaluate(X_test,y_test_oh)\n",
    "    print(f\"--------------- {size} --------------------\")\n",
    "    print(\"Categorical cross-entropy:\", metrics[0])\n",
    "    print(\"Accuracy:\", metrics[1])\n",
    "\n",
    "    out_sizes.append(size)\n",
    "    out_accuracies.append(metrics[1])\n",
    "    out_size_increases.append(size_increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df to add to file\n",
    "extra_df = pd.DataFrame({\n",
    "        \"original_size\":out_sizes,\n",
    "        \"size_increase\":out_size_increases,\n",
    "        \"total_size\":target_size,\n",
    "        \"accuracy\":out_accuracies,\n",
    "        \"training_epochs\":epoch_count,\n",
    "        \"model\":classifier_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to file\n",
    "save_entry(\"10000_fixed.csv\",extra_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f1fa92065e83bf8b3f9b5096216ce92755ea229f7fd6780b8b3e0f199343520"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
